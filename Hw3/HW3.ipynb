{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NAME = \"Akshay Shah\"\n",
    "COLLABORATORS = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%run datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (5.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (0.23)\n",
      "Requirement already satisfied: wcwidth in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (0.1.7)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (7.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (1.3.0)\n",
      "Requirement already satisfied: packaging in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (1.8.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from pytest) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from packaging->pytest) (2.4.2)\n",
      "Requirement already satisfied: six in /Users/akshah/virtualenvs/fsl_env/lib/python3.6/site-packages (from packaging->pytest) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78a822b052779b35ccfc9bbdba6ed1fa",
     "grade": false,
     "grade_id": "cell-1a02bff32a097b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As part of this assignment, we will be learning to model a  handwritten digit classifier using Multi-layer Neural Network. The model will be trained to classify the images of handwritten digits into 10 classes( digits 0 -9).\n",
    "We will be using [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/) for training the classifier.The dataset is a good example of real-world data and is widely used by Machine Learning community for learning techniques and pattern recognition methods.\n",
    "\n",
    "MNIST dataset contains grayscale samples of handwritten digits of size 28 $\\times$ 28. It is split into training set of 60,000 examples, and a test set of 10,000 examples. For this assignment, we will using a smaller subset of 1500 training samples, 500 validation samples and 1000 test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9ad732b79c383cf625527f65cc515e3",
     "grade": false,
     "grade_id": "cell-9ce83fefdb3ee268",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Libraries\n",
    "\n",
    "As first step, let us import the required datasets and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9b003832d58ae12e33880c29809261",
     "grade": false,
     "grade_id": "cell-d9bd60ff8a7a5aba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import pytest\n",
    "import random\n",
    "import numpy.matlib \n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "110f3f6c3228bc50d31aa03c64112f3b",
     "grade": false,
     "grade_id": "cell-7e789c2d07d0df38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_samples = 1500\n",
    "val_samples = 500\n",
    "test_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "891a4ea08076fdd3cb16a167705c9168",
     "grade": false,
     "grade_id": "cell-153e3e96f279c5f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "digits = list(range(10))\n",
    "trX, trY, tsX, tsY, valX, valY = mnist(train_samples,val_samples,test_samples, digits=digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e78f3f2cfe29e9ea7d4df1c819907fa",
     "grade": false,
     "grade_id": "cell-77306ff2cf41f0a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Visualize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37a3d6c9d54d90b81e6e968726cc4d1e",
     "grade": false,
     "grade_id": "cell-2a78065ebe04febb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize few samples and their labels from the train and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6745c14cbaaf32430f6879aa17cf210b",
     "grade": false,
     "grade_id": "cell-59ae07e5133951ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(trX[:,0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e57bddc23bbae0ad2442536bd1c92b9",
     "grade": false,
     "grade_id": "cell-02ddd6a3385d6cba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trY[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4b451acb56325deb6fe03500786cab",
     "grade": false,
     "grade_id": "cell-f7b6a0f5ffc42b7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(tsX[:,100].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d863265473c39235b56b636feef6c0d4",
     "grade": false,
     "grade_id": "cell-10dbdc9668e7e0aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tsY[:,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(valX[:,0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valY[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d4471bd13cba26906ba1a2eb6b198d3",
     "grade": false,
     "grade_id": "cell-e305de55136992d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can split the assignment into 3 sections.\n",
    "\n",
    "## Section 1  \n",
    "We will build the primary components of a Neural network. We will define the activation functions and their derivatives which will be used later during forward and backward propagation. We will also define Dropout for regularization. Finally We will define the softmax cross entropy loss for calculating the prediction loss.\n",
    "\n",
    "## Section 2\n",
    "In section 2, We will use the components from section 1 to define forward and back propagations. We will initially perform a single forward/backward pass and extend it to multi-layer network.\n",
    "\n",
    "## Section 3\n",
    "In the final section, We will assemble all the components into a single Neural Network as shown below. We will train the model to classify the images into 10 classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99c1adec06edb95b5ca0ae8c4c6cd584",
     "grade": false,
     "grade_id": "cell-504361fa436fb08d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![Architechture](images/MLP_architechture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4937346aa47ec0474a6325f9a2d26f8b",
     "grade": false,
     "grade_id": "cell-517724a712cbaa66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4275ca6898012f905062ebd1ee1622c",
     "grade": false,
     "grade_id": "cell-b4d083748aff2c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Parameter Initialization (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36ec8fbb7e909664ae9aec61c1da3146",
     "grade": false,
     "grade_id": "cell-2d445d2fe1bb530d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now define a function that can initialize the parameters of the Neural Network.\n",
    "The network parameters are wrapped as dictionary elements that can easily be passed as function parameters while calculating gradients during back propogation.\n",
    "\n",
    "1. The weight matrix is initialized with random values from a normal distribution of variance $1$. For example, to create a matrix $W$ of dimension $3 \\times 4$, with values from a normal distribution with variance $1$,\n",
    "we define $W = np.random.normal(size =(3,4))$.\n",
    "\n",
    "2. Bias values are initialized with a vector of 0's.\n",
    "\n",
    "The dimension of weight matrix for a layer $(l+1)$ is given by ( Number of neurons in $(l+1)$  X  Number of neurons in $l$ )\n",
    "\n",
    "The dimension of bias vector for a layer $(l+1)$ is given by ( Number of neurons in $(l+1)$  X  Number of neurons in $1$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33403341fbd4207cb88d4c25681fece8",
     "grade": false,
     "grade_id": "cell-e65e007fab5dfe94",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize(net_dims):\n",
    "    '''\n",
    "    Inputs:\n",
    "    \n",
    "    net_dims - Array containing the dimensions of the network. The values of the array represent the number of nodes in \n",
    "    each layer. For Example, if a Neural network contains 784 nodes in the input layer , 800 in the first hidden layer,\n",
    "     500 in the secound hidden layer and 10 in the output layer, then net_dims =[784,800,500,10]. \n",
    "    \n",
    "    Outputs:\n",
    "    parameters - Dictionary element for storing the Weights and bias of each layer of the network\n",
    "    '''\n",
    "    numLayers = len(net_dims)\n",
    "    parameters = {}\n",
    "    for l in range(numLayers-1):\n",
    "        # Hint:    \n",
    "        # parameters[\"W\"+str(l+1)] = \n",
    "        # parameters[\"b\"+str(l+1)] =\n",
    "        # YOUR CODE HERE\n",
    "        parameters[\"W\"+str(l+1)] = np.random.normal(size = (net_dims[l+1],net_dims[l]))\n",
    "        parameters[\"b\"+str(l+1)] = np.ones((net_dims[l+1],1))\n",
    "        \"\"\"\n",
    "        Check the output layer dimension for W it should be 10 x 500?\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adab134f9ae35918f9944e3d8c88402b",
     "grade": true,
     "grade_id": "test_case1_initialize",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test \n",
    "net_dims_tst = [5,4,1]\n",
    "parameters_tst = initialize(net_dims_tst)\n",
    "assert parameters_tst['W1'].shape == (4,5)\n",
    "assert parameters_tst['b1'].shape == (4,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "295b7033ac382e07cb9735d5ae2ba642",
     "grade": false,
     "grade_id": "cell-e77c10bfbde99f2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8932856ef998dffe533aae104ca7ca83",
     "grade": false,
     "grade_id": "cell-1b4d8330f961f447",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As discussed in the lecture, An Activation function takes an input from the previous layer and performs a certain fixed mathematical operation and the result is passed to the following layer.\n",
    "1. ReLU or Rectified Linear Unit\n",
    "2. Linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "098793bd6f778922d5b2654a4184e679",
     "grade": false,
     "grade_id": "cell-36f264bd171e66a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ReLU (Rectified Linear Unit) (5 points)\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a piecewise linear function that will output the input if is positive, otherwise, it's output is zero.\n",
    "\n",
    "\\begin{equation*}\n",
    "ReLU(x) = Max(0,x)\n",
    "\\end{equation*}\n",
    "\n",
    "Hint: use [numpy.maximum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47dbfc98067f948a922df42b92cce49c",
     "grade": false,
     "grade_id": "cell-cef617bf7022bb68",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    Computes relu activation of Z\n",
    "    \n",
    "    Inputs: \n",
    "        Z is a numpy.ndarray (n, m) which represent 'm' samples each of 'n' dimension\n",
    "        \n",
    "    Returns: \n",
    "        A is activation. numpy.ndarray (n, m) representing 'm' samples each of 'n' dimension\n",
    "        cache is a dictionary with {\"Z\", Z}\n",
    "        \n",
    "    '''\n",
    "    cache = {}\n",
    "    # YOUR CODE HERE\n",
    "    A = np.maximum(0,Z)\n",
    "    cache[\"Z\"] = Z\n",
    "    #raise NotImplementedError()\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2101d17d365a68cb22333764c84bc039",
     "grade": true,
     "grade_id": "test_case2_relu",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "z_tst = np.array([[-1,2],[3,-6]])\n",
    "a_tst, c_tst = relu(z_tst)\n",
    "npt.assert_array_equal(a_tst,[[0,2],[3,0]])\n",
    "assert (c_tst[\"Z\"] == np.array([[-1,2],[3,-6]])).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca3454bc892ce52db8557ecd223a6a7d",
     "grade": false,
     "grade_id": "cell-9e69ac398fc920e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ReLU - Gradient\n",
    "\n",
    "The gradient of ReLu is a simple unit function with output 0 for all values less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "846a74f8cdab72958ecf59f64848a4fd",
     "grade": false,
     "grade_id": "cell-dfcfa7766ff3f088",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of relu activation\n",
    "    \n",
    "    Inputs: \n",
    "        dA is the derivative from the upstream layer with dimensions (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
    "        to the activation layer during forward propagation\n",
    "        \n",
    "    Returns: \n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "        \n",
    "    '''\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    Z = cache[\"Z\"]\n",
    "    dZ = np.where(dZ < 0, 0,dZ)\n",
    "    #raise NotImplementedError()\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a58947362bba52e4d12d95977c897c7",
     "grade": true,
     "grade_id": "cell-7d037a6a7c05a13f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test`\n",
    "dA_tst = np.array([[-7,5],[2,-3]])\n",
    "cache_tst ={}\n",
    "cache_tst[\"Z\"] = np.array([[-1,1],[0,-3]])\n",
    "dZ_tst = relu_der(dA_tst,cache_tst)\n",
    "npt.assert_array_equal(dZ_tst,np.array([[0,5],[2,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5efb9e479d523be82946f9ae08c22300",
     "grade": false,
     "grade_id": "cell-c8886cbc2ede8019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear\n",
    "\n",
    " Linear activation performs a simple linear operation of passing the input.\n",
    "\\begin{equation*}\n",
    "Linear(x) = x\\\\\n",
    "dx = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b337b5337170e8d806cdb09836ec4f68",
     "grade": false,
     "grade_id": "cell-5c19d5fd5d97fb3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    '''\n",
    "    computes linear activation of Z\n",
    "    This function is implemented for completeness\n",
    "    Inputs: \n",
    "        Z is a numpy.ndarray (n, m)\n",
    "        \n",
    "    Returns: \n",
    "        A is activation. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}   \n",
    "    '''\n",
    "    A = Z\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58983c12564bf1d2e0e42c647272430a",
     "grade": false,
     "grade_id": "cell-f571cb53ebd8e3e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of linear activation\n",
    "    This function is implemented for completeness\n",
    "    \n",
    "    Inputs: \n",
    "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
    "        to the activation layer during forward propagation\n",
    "    \n",
    "    Returns: \n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "    '''      \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99dc31bceb934b3b811d9a2a0e854d3e",
     "grade": false,
     "grade_id": "cell-076c0de6c87fa8af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Loss function (5 Points)\n",
    "\n",
    "The softmax activation is computed on the outputs from the last layer and the output label with the maximum probablity is predicted as class label. The softmax function can also be refered as  normalized exponential function which takes a vector of K real numbers as input, and normalizes it into a probability distribution consisting of $n$ probabilities proportional to the exponentials of the input numbers.\n",
    "\n",
    "The input to the softmax function is the matrix of all the samples, $ Z = [ z^{(1)} , z^{(2)}, \\ldots, z^{(m)} ] $, where $z^{(i)}$ is the $i^{th}$ sample of $n$ dimensions. We estimate the softmax for each of the samples $1$ to $m$. The softmax activation for $a^{(i)} = \\text{softmax}(z^{(i)})$ is, \n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{exp(z^{(i)}_k)}{\\sum_{k = 1}^{n}exp(z^{(i)}_k)} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "The output of the softmax is $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, where $a^{(i)} = [a^{(i)}_1,a^{(i)}_2, \\ldots, a^{(i)}_n]^\\top$.  In order to avoid floating point overflow, we subtract a constant from all the input components of $z^{(i)}$ before calculating the softmax. This constant is $z_{max}$, where, $z_{max} = max(z_1,z_2,...z_n)$. The activation is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{exp(z^{(i)}_k- z_{max})}{\\sum_{k = 1}^{n}exp(z^{(i)}_k - z_{max})} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "If the output of softmax is given by $A$ and the ground truth is given by $Y = [ y^{(1)} , y^{(2)}, \\ldots, y^{(m)}]$, the cross entropy loss between the predictions $A$ and groundtruth labels $Y$ is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "Loss(A,Y) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=0}^{n-1}I \\{ y^i = k \\} \\text{log}a_k^i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $I$ is the identity function given by \n",
    "\n",
    "\\begin{equation}\n",
    "I\\{\\text{condition}\\} = 1, \\quad \\text{if condition = True}\\\\\n",
    "I\\{\\text{condition}\\} = 0, \\quad \\text{if condition = False}\\\\\n",
    "\\end{equation}\n",
    "Hint: use [numpy.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)\n",
    "numpy.max,\n",
    "[numpy.sum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)\n",
    "[numpy.log](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)\n",
    "Also refer to use of 'keepdims' and 'axis' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "753f20cb12f4c799f30d802baf737dfc",
     "grade": false,
     "grade_id": "cell-2c4d26539c997506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
    "    '''\n",
    "    Computes the softmax activation of the inputs Z\n",
    "    Estimates the cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Z - numpy.ndarray (n, m)\n",
    "        Y - numpy.ndarray (d, m) - labels in one-hot representation\n",
    "            when y=[] loss is set to []\n",
    "    \n",
    "    Returns:\n",
    "        A - numpy.ndarray (n, m) of softmax activations\n",
    "        cache -  a dictionary to store the activations later used to estimate derivatives\n",
    "        loss - cost of prediction\n",
    "    \n",
    "    http://cs231n.github.io/neural-networks-case-study/\n",
    "    https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays\n",
    "    '''\n",
    "    #Testing y\n",
    "    #y = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "    \n",
    "    m = Z.shape[1]\n",
    "    n = Z.shape[0]\n",
    "    z_max = np.max(Z)\n",
    "    exp_values = np.exp(Z - z_max)\n",
    "    A = exp_values / np.sum(exp_values,axis=0,keepdims=True)\n",
    "    print(A,A.shape)\n",
    "    #Y_args = np.argmax(Y,axis=0)\n",
    "    #loss = np.sum(np.log(A[]))\n",
    "#     y_arg = np.argmax(y,axis=0).reshape(1,m)\n",
    "#     print(y_arg,y_arg.shape)\n",
    "#     m_arg = np.arange(m).reshape(1,m)\n",
    "#     print(m_arg,m_arg.shape)\n",
    "    \n",
    "#     loss = (-1/m) * np.sum(A[y_arg,m_arg])\n",
    "#     print(loss)\n",
    "    \n",
    "    loss = 0.0\n",
    "    if not len(Y):\n",
    "        loss = []\n",
    "    else:\n",
    "        for i in range(m):\n",
    "            loss += np.divide(np.multiply(Y[i],np.log(A[i]),m))\n",
    "    \n",
    "    #print(A[range(m),y])\n",
    "    #loss = -1\n",
    "\n",
    "#raise NotImplementedError()\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return A, cache, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "711547c3efef40b9bc56e6236395ce27",
     "grade": true,
     "grade_id": "test_case4_softmax",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04742587 0.26894142 0.98201379]\n",
      " [0.95257413 0.73105858 0.01798621]] (2, 3)\n"
     ]
    }
   ],
   "source": [
    "A_tst, _ ,_ = softmax_cross_entropy_loss(np.array([[-1,0,1],[2,1,-3]]))\n",
    "npt.assert_almost_equal(np.sum(A_tst),3,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e00ffdab895ed2b7ab151bbc48107d77",
     "grade": false,
     "grade_id": "cell-c0c8456041e2d2ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# One-hot representation\n",
    "\n",
    "One-hot encoding is a vector representation of the class labels, where all the elements of the vector, except the  class being represented, is zero. This representation will be used to identify the prediction label and calculate the loss of the network.\n",
    "\n",
    "\n",
    "Let the number of categories be $K=3$. Let ground truth labels be $Y = [1,3,2,1]$. Then the one-hot representation of $Y$ is \n",
    "\\begin{equation}\n",
    "\\bar{Y} = \n",
    "    \\begin{bmatrix}\n",
    "    1 ~ 0 ~ 0 ~ 1\\\\\n",
    "    0 ~ 0 ~ 1 ~ 0\\\\\n",
    "    0 ~ 1 ~ 0 ~ 0\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "where, the one-hot encoding for label $y^{(1)} = 1$ is $\\bar{y}^{(1)} = [1, 0, 0]^\\top$. Similarly, the one-hot encoding for $y^{(2)} = 3$ is $\\bar{y}^{(2)} = [0, 0, 1]^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7daa700985b7c41af2c1349fb4966c28",
     "grade": false,
     "grade_id": "cell-8d591ce79a731d94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(Y,num_classes):\n",
    "    '''\n",
    "    Return one hot vector for the lables\n",
    "\n",
    "    Inputs:\n",
    "        Y - Labels of dimension (1,m)\n",
    "        num_classes - Number of output classes\n",
    "        \n",
    "    Ouputs:\n",
    "        Y_one_hot - one hot vector of dimension(n_classes,m)\n",
    "    '''\n",
    "    Y_one_hot = np.zeros((num_classes,Y.shape[1]))\n",
    "    for i in range(Y.shape[1]):\n",
    "        Y_one_hot[int(Y[0,i]),i] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eab9c6fe6b7f5ab6d771d2c392a0cf8f",
     "grade": false,
     "grade_id": "cell-5151a9f9720ee789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Derivative of the Loss Function(5 points)\n",
    "\n",
    "The derivative of the multiclass cross entropy loss can be given as the difference between the Activation output and ground truth. If $A$ is vector of $m$ samples , as $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, the gradient of softmax is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "dZ =\\frac{1}{m} (A -Y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dae974288ede73f2d8746b2ed5380ca",
     "grade": false,
     "grade_id": "cell-7af89efa4d354b07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss_der(Y, cache):\n",
    "    '''\n",
    "    Computes the derivative of softmax activation and cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Y - numpy.ndarray (n,m) is a one-hot encoding of the ground truth labels\n",
    "        cache -  a dictionary with cached activations A of size (n,m)\n",
    "\n",
    "    Returns:\n",
    "        dZ - numpy.ndarray (n, m) derivative for the previous layer\n",
    "    '''\n",
    "    A = cache[\"A\"]\n",
    "    dZ = (1/Y.shape[1]) * np.subtract(A,Y)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af1fd33dadb2df48e9ff76fbddb862ba",
     "grade": true,
     "grade_id": "test_case5_softmax_der",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "c_tst = {'A':np.array([[0.4,0.4],[0.6,0.6]])}\n",
    "Y_tst = np.array([[0,1],[1,0]])\n",
    "npt.assert_array_almost_equal(softmax_cross_entropy_loss_der(Y_tst, c_tst),np.array([[0.2,-0.3],[-0.2,0.3]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Drop Out (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6117588ccffc86b971e5c6f2b4fc2e1",
     "grade": false,
     "grade_id": "cell-0fa31a4b61b6aedf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout_forward(A, drop_prob, mode='train'):\n",
    "        '''\n",
    "        Use the 'inverted dropout' technique to implement dropout regularization.\n",
    "        Inputs:\n",
    "                drop_prob - dropout parameter, we drop the number of neurons in a given layer with respect to prob.\n",
    "                A - Activation output\n",
    "                mode - Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "        takes in only 2 values, 'train' or 'test'\n",
    "\n",
    "        Outputs:\n",
    "                out - Output of shape(n,m) same as input but with some values masked out.\n",
    "                cache - a tuple which stores the values that are required in the backward pass.\n",
    "        '''\n",
    "        mask = None\n",
    "        out = None\n",
    "        pkeep = 1 - drop_prob\n",
    "        if mode == 'train':\n",
    "            #for i in range(A.shape):\n",
    "            mask = np.random.rand(*A.shape) < pkeep\n",
    "            out = (A * mask) / pkeep\n",
    "            #out = out / pkeep\n",
    "            out = out.reshape(A.shape)\n",
    "#raise NotImplementedError()\n",
    "        elif mode == 'test':\n",
    "            out = A\n",
    "#raise NotImplementedError()\n",
    "        else:\n",
    "            raise ValueError(\"Mode value not set, set it to 'train' or 'test'\")\n",
    "        cache = (mode, mask)\n",
    "        out = out.astype(A.dtype, copy=False)\n",
    "        return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52127751 0.90040562 0.         0.37791572 0.         0.11542324\n",
      "  0.23282526 0.         0.49595934 0.        ]\n",
      " [0.52399314 0.85652438 0.         1.0976468  0.03423449 0.83808439\n",
      "  0.521631   0.         0.17548367 0.24762686]\n",
      " [1.00093071 1.21032697 0.39178022 0.86540327 0.         1.11825833\n",
      "  0.10630526 0.         0.21228802 1.09767813]\n",
      " [0.12293354 0.         1.19736191 0.66645661 0.         0.39439454\n",
      "  0.         1.04328209 0.         0.93768039]\n",
      " [1.23607636 0.93520707 0.35055499 0.98659916 0.12903251 0.55986691\n",
      "  1.13574438 0.36701769 0.35971917 0.16253572]\n",
      " [0.0242087  0.         0.26453515 0.33193332 0.61446645 0.06670318\n",
      "  0.71764701 0.18341072 0.         0.        ]\n",
      " [0.12791804 0.51756998 0.8680002  0.         0.06244182 0.66987051\n",
      "  0.82974331 0.64361139 1.18074344 0.7331938 ]\n",
      " [1.12925239 0.         0.17409543 1.00923911 0.49709605 0.20669275\n",
      "  1.15938573 0.43470732 0.93851513 0.90749748]\n",
      " [1.10413261 0.77959026 0.93867804 0.43612293 0.33740986 0.\n",
      "  0.53511399 1.20605006 0.82930187 0.        ]\n",
      " [0.14343247 1.18686157 0.56239017 0.72298702 0.         0.\n",
      "  1.1292244  0.71709936 0.00358791 0.        ]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n ACTUAL: 48.45092629634842\n DESIRED: 45.704208",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-21a2383fc397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45.704208\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/fsl_env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n ACTUAL: 48.45092629634842\n DESIRED: 45.704208"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "x = np.random.rand(10,10)\n",
    "drop_prob = 0.2\n",
    "out, cache = dropout_forward(x, drop_prob, mode='train')\n",
    "print(out)\n",
    "npt.assert_almost_equal(np.sum(out),45.704208,6)\n",
    "\n",
    "out, cache = dropout_forward(x, drop_prob, mode='test')\n",
    "npt.assert_almost_equal(np.sum(out), 48.587792, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15d38c455a09203600d94d552d6848d5",
     "grade": true,
     "grade_id": "test_case_dropout",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n ACTUAL: 48.45092629634842\n DESIRED: 45.704208",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-623299653bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrop_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45.704208\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/fsl_env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n ACTUAL: 48.45092629634842\n DESIRED: 45.704208"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "x = np.random.rand(10,10)\n",
    "drop_prob = 0.2\n",
    "out, cache = dropout_forward(x, drop_prob, mode='train')\n",
    "npt.assert_almost_equal(np.sum(out),45.704208,6)\n",
    "\n",
    "out, cache = dropout_forward(x, drop_prob, mode='test')\n",
    "npt.assert_almost_equal(np.sum(out), 48.587792, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c761116ffb0a379358059c0ad956dc8",
     "grade": false,
     "grade_id": "cell-c724edbcb50ce841",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout_backward(cache, dout):\n",
    "        '''\n",
    "        Backward pass for the inverted dropout.\n",
    "        Inputs: \n",
    "              dout: derivatives from the upstream layers of dimension (n,m).\n",
    "              cache: contains the mask, input, and chosen dropout probability from the forward pass.\n",
    "        Outputs:\n",
    "              dA = derivative from the layer of dimension (n,m)\n",
    "        '''\n",
    "        dA = None\n",
    "        mode, mask = cache\n",
    "        if mode == 'train':\n",
    "            dA = np.multiply(dout,mask)\n",
    "        elif mode == 'test':\n",
    "            dA = dout\n",
    "        else:\n",
    "            raise ValueError(\"Mode value not set, set it to 'train' or 'test'\")\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n ACTUAL: 2.273178977617041\n DESIRED: 1.385093",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3f9008e8816c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.385093\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtualenvs/fsl_env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n ACTUAL: 2.273178977617041\n DESIRED: 1.385093"
     ]
    }
   ],
   "source": [
    "dout = np.random.rand(3,2)\n",
    "mask = np.random.rand(3,2)\n",
    "mode = 'test'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "#npt.assert_almost_equal(np.sum(dA),3.9176559,6)\n",
    "\n",
    "mode = 'train'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "npt.assert_almost_equal(np.sum(dA),1.385093,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d8fa30aa85561a40e4bf18391e24004",
     "grade": true,
     "grade_id": "cell-132946bbbfe2505e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n ACTUAL: 4.495817063476455\n DESIRED: 3.9176559",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-558b9d24a449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.9176559\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/fsl_env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n ACTUAL: 4.495817063476455\n DESIRED: 3.9176559"
     ]
    }
   ],
   "source": [
    "dout = np.random.rand(3,2)\n",
    "mask = np.random.rand(3,2)\n",
    "mode = 'test'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "npt.assert_almost_equal(np.sum(dA),3.9176559,6)\n",
    "\n",
    "mode = 'train'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "npt.assert_almost_equal(np.sum(dA),1.385093,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "dout = np.random.rand(3,2)\n",
    "mask = np.random.rand(3,2)\n",
    "mode = 'test'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "npt.assert_almost_equal(np.sum(dA),1.6788879311798277,6)\n",
    "\n",
    "mode = 'train'\n",
    "cache = (mode, mask)\n",
    "dA = dropout_backward(cache, dout)\n",
    "npt.assert_almost_equal(np.sum(dA),0.61432916214326,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c834a6571465d1b6159bc22b290348f",
     "grade": false,
     "grade_id": "cell-e9465c382fdd4cfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e410ee992a171c0777296bd92846fe73",
     "grade": false,
     "grade_id": "cell-36236b2868ecda30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Forward Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b37d953967e745115023926ae69cd414",
     "grade": false,
     "grade_id": "cell-004fcac93e6f7ece",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## One Layer\n",
    "  \n",
    "If the vectorized input to any layer of neural network is $A$ and the parameters of the layer is given by $(W,b)$ ,the output of the layer is,\n",
    "\\begin{equation}\n",
    "Z = W A + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86a20cd5512aad02522142a334028ae9",
     "grade": false,
     "grade_id": "cell-8a69a105c7d1dd23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    Input A propagates through the layer \n",
    "    Z = WA + b is the output of this layer. \n",
    "\n",
    "    Inputs: \n",
    "        A - numpy.ndarray (n,m) the input to the layer\n",
    "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
    "\n",
    "    Returns:\n",
    "        out = dropout(WA + b), where out is the numpy.ndarray (n_out, m) dimensions\n",
    "        cache - a dictionary containing the inputs A\n",
    "    '''\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return Z, cache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3735e8d3ff33792768fa9a89f8a3cfdc",
     "grade": false,
     "grade_id": "cell-3c463f4362ff9844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Layer + Activation\n",
    "In addition to layer, the following function also computes the activation of each layer which is given by,\n",
    "\\begin{equation}\n",
    "Z = W X + b\\\\\n",
    "A = \\sigma (Z)\n",
    "\\end{equation}\n",
    "\n",
    "depending on the activation choosen for the given layer, the $\\sigma(.)$ can represent different operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae7871bd2f5513aa5668989240e6ee44",
     "grade": false,
     "grade_id": "cell-f70a31ac49d9bb4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_forward(A_prev, W, b, activation, drop_prob, mode):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer and the activation\n",
    "\n",
    "    Inputs: \n",
    "        A_prev - numpy.ndarray (n,m) the input to the layer\n",
    "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
    "        activation - is the string that specifies the activation function\n",
    "\n",
    "    Returns:\n",
    "        A = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        g is the activation function\n",
    "        cache - a dictionary containing the cache from the linear and the nonlinear propagation\n",
    "        to be used for derivative\n",
    "    '''\n",
    "\n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, act_cache = relu(Z)\n",
    "        A, drop_cache =  dropout_forward(A, drop_prob, mode)\n",
    "        \n",
    "    elif activation == \"linear\":\n",
    "        A, act_cache = linear(Z)\n",
    "        drop_cache = None\n",
    "        \n",
    "\n",
    "    cache = {}\n",
    "    cache[\"lin_cache\"] = lin_cache\n",
    "    cache[\"act_cache\"] = act_cache\n",
    "    cache[\"drop_cache\"] = drop_cache\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2219461d7441672ff084a67aa7a95c12",
     "grade": false,
     "grade_id": "cell-243ec2c02ff1834c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multi-Layers (5 Points)\n",
    "\n",
    "The forward layers are stacked to form a multi layer network. The number of layers used by the network can be inferred from the size of the $parameters$. If the number of items in the dictionary element $parameters$ is $2L$, then the number of layers will be $L$\n",
    "\n",
    "During forward propagation , the input sample $A_0$ is fed into the first layer and the subsequent layers use the activation output from the previous layer as inputs.\n",
    "\n",
    "Please note all the hidden layers use **ReLU** activation except the last layer which uses **Linear** activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb64b38c9fc316daf85954310cf3e1d",
     "grade": false,
     "grade_id": "cell-40ada61852849730",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_forward(X, parameters,drop_prob, mode):\n",
    "    '''\n",
    "    Forward propgation through the layers of the network\n",
    "\n",
    "    Inputs: \n",
    "        X - numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters - dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "    Returns:\n",
    "        AL - numpy.ndarray (c,m)  - outputs of the last fully connected layer before softmax\n",
    "            where c is number of categories and m is number of samples in the batch\n",
    "        caches - a dictionary of associated caches of parameters and network inputs\n",
    "    '''\n",
    "    L = len(parameters)//2  \n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A, cache = layer_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"relu\",drop_prob, mode)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = layer_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"linear\",drop_prob, mode)\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5acc1ee9041bb40e55ccbc7abddca04a",
     "grade": true,
     "grade_id": "test_case7_multilayer",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tst = np.array([[1,3,2,5],[2,4,-2,6]])\n",
    "param_tst ={'W1':[1,2],'b1':1}\n",
    "drop_prob = 0.33\n",
    "mode = 'test'\n",
    "AL_t,c_t= multi_layer_forward(X_tst, param_tst, drop_prob, mode)\n",
    "npt.assert_array_almost_equal(AL_t,np.array([ 6, 12, -1, 18]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abb3ca307284688bd338e4044a3206d",
     "grade": false,
     "grade_id": "cell-5281900868c84d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Backward Propagagtion (10 Points)\n",
    "\n",
    "Let us now perform back propagation through the layers and calculate the gradients of the network parameters $(dW,db)$.\n",
    "\n",
    "If the derivative of the loss$\\frac{dL}{dZ}$ is given as $dZ$ and network paramerters are given as $(W,b)$, the gradients $(dW,db)$ can be calculated as,\n",
    "\n",
    "\\begin{equation}\n",
    "dA_{prev} = W^T dZ\\\\\n",
    "dW = dZ A^T\\\\\n",
    "db = \\sum_{i=1}^{m} dz^{(i)}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "where $dZ = [dz^{(1)},dz^{(2)}, \\ldots, dz^{(m)}]$ is the column vector of the gradient of loss in the kth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "144ce6f098a862cfc2fa51c53752f434",
     "grade": false,
     "grade_id": "cell-5ae7b7d7ae72810a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, W, b):\n",
    "    '''\n",
    "    Backward prpagation through the linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dZ - numpy.ndarray (n,m) derivative dL/dz \n",
    "        cache - a dictionary containing the inputs A, for the linear layer\n",
    "            where Z = WA + b,    \n",
    "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
    "        W - numpy.ndarray (n,p)\n",
    "        b - numpy.ndarray (n, 1)\n",
    "\n",
    "    Returns:\n",
    "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW - numpy.ndarray (n,p) the gradient of W \n",
    "        db - numpy.ndarray (n, 1) the gradient of b\n",
    "    '''      \n",
    "    A = cache['A']\n",
    "    # YOUR CODE HERE\n",
    "    dA_prev = W.T @ dZ\n",
    "    dW = dZ @ A.T\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b68ff43cd04737f4a19c5dff8069c3b1",
     "grade": true,
     "grade_id": "cell-22a9c03f836d3b76",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cache_test ={}\n",
    "A_tst = np.array([[1,3,2,5],[2,4,-2,6]])\n",
    "cache_test['A'] = A_tst\n",
    "W_tst = np.array([[3,2,1,-1],[1,3,1,3]])\n",
    "b_tst = np.array([1,1]).reshape(-1,1)\n",
    "dZ_tst = np.array([[0.1,0.3,0.1,.7],[.2,.3,0.3,.5]])\n",
    "\n",
    "dA_prev,dW_tst,db_tst = linear_backward(dZ_tst, cache_test, W_tst, b_tst)\n",
    "npt.assert_almost_equal(np.sum(dA_prev),16.4)\n",
    "npt.assert_array_almost_equal(dW_tst,np.array([[4.7, 5.4],[4.2, 4. ]]))\n",
    "npt.assert_array_almost_equal(db_tst,np.array([[1.2],[1.3]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f10dc38e0f1df8505608ac53c7ec3503",
     "grade": false,
     "grade_id": "cell-f57dc4108dd56c38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Layer + Activation\n",
    "\n",
    "In the below function, we also account for the activation while calculating the derivative.\n",
    "We use the derivative functions defined earlier to calculate $(\\frac{dL}{dZ})$ followed by back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b70a186450e315956bc74fa55033b9bf",
     "grade": false,
     "grade_id": "cell-9b7cfa89255f3e03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_backward(dA, cache, W, b, activation):\n",
    "    '''\n",
    "    Backward propagation through the activation and linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dA - numpy.ndarray (n,m) the derivative to the previous layer\n",
    "        cache - dictionary containing the linear_cache and the activation_cache\n",
    "        activation - activation of the layer\n",
    "        W - numpy.ndarray (n,p)\n",
    "        b - numpy.ndarray (n, 1)\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW - numpy.ndarray (n,p) the gradient of W \n",
    "        db - numpy.ndarray (n, 1) the gradient of b\n",
    "    '''\n",
    "    lin_cache = cache[\"lin_cache\"]\n",
    "    act_cache = cache[\"act_cache\"]\n",
    "    drop_cache = cache[\"drop_cache\"]\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dA = dropout_backward(drop_cache, dA)\n",
    "        dZ = relu_der(dA, act_cache)\n",
    "        \n",
    "    elif activation == \"linear\":\n",
    "        \n",
    "        dZ = linear_der(dA, act_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8472d5f6683479493cd030a9583d3c2e",
     "grade": false,
     "grade_id": "cell-72a9dc0cb265dc90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multi-layers\n",
    "\n",
    "We have defined the required functions to handle back propagation for single layer. Now we will stack the layers together and perform back propagation on the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f7f5d5b06322031e9efa80798fc51c7",
     "grade": false,
     "grade_id": "cell-8d2141e7c67dafa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_backward(dAL, caches, parameters):\n",
    "    '''\n",
    "    Back propgation through the layers of the network (except softmax cross entropy)\n",
    "    softmax_cross_entropy can be handled separately\n",
    "\n",
    "    Inputs: \n",
    "        dAL - numpy.ndarray (n,m) derivatives from the softmax_cross_entropy layer\n",
    "        caches - a dictionary of associated caches of parameters and network inputs\n",
    "        parameters - dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "\n",
    "    Returns:\n",
    "        gradients - dictionary of gradient of network parameters \n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "    '''\n",
    "\n",
    "    L = len(caches) \n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    activation = \"linear\"\n",
    "    for l in reversed(range(1,L+1)):\n",
    "        dA, gradients[\"dW\"+str(l)], gradients[\"db\"+str(l)] = \\\n",
    "                    layer_backward(dA, caches[l-1], \\\n",
    "                    parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\\\n",
    "                    activation)\n",
    "        activation = \"relu\"\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b46616906e65178bc046bf29b3f045f",
     "grade": false,
     "grade_id": "cell-66defde04ecba045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Prediction (5 Points)\n",
    "\n",
    "Let us now assemble the different parts of forward propagation into a single unit and use the ouput to make a prediction.\n",
    "\n",
    "Step 1 - Forward propagate X using multi_layer_forward and obtain the activation 'A'.<br>\n",
    "Step 2 - Using 'softmax_cross_entropy loss', obtain softmax activation of last layer.<br>\n",
    "Step 3 - Class label 'Ypred' is predicted as the 'argmax' of the softmax activation from step 2.<br>\n",
    "Note: the shape of 'YPred' is (1,m), where m is the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "073c0fb32953510c6c55d1e859f04ff9",
     "grade": false,
     "grade_id": "cell-ffdb132c59706ff2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classify(X, parameters,mode,drop_prob):\n",
    "    '''\n",
    "    Network prediction for inputs X\n",
    "\n",
    "    Inputs: \n",
    "        X - numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters - dictionary of network parameters \n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "    Returns:\n",
    "        YPred - numpy.ndarray (1,m) of predictions\n",
    "    '''\n",
    "    # Forward propagate input 'X' using multi_layer_forward and obtain the activation 'A'\n",
    "    # Using 'softmax_cross_entropy loss', obtain softmax activation of last layer with input 'A' from step 1\n",
    "    # Predict class label 'YPred' as the 'argmax' of softmax activation from step-2. \n",
    "    # Note: the shape of 'YPred' is (1,m), where m is the number of samples\n",
    "\n",
    "    A, caches = multi_layer_forward(X, parameters,drop_prob, mode)\n",
    "    Z, _ ,_ =softmax_cross_entropy_loss(A, Y=np.array([]))\n",
    "    YPred = np.argmax(Z,axis=0).reshape(1,-1)\n",
    "    return YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0c20526bf4915b6c19e8cefcbef534",
     "grade": true,
     "grade_id": "test_case8_classify",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.53978687e-05 9.93307149e-01 4.53978687e-05]\n",
      " [9.99954602e-01 6.69285092e-03 9.99954602e-01]] (2, 3)\n"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "drop_prob = 0.2\n",
    "X_tst = np.array([[-1,2,1],[1,1,3]])\n",
    "param_tst ={'W1':5,'b1':3}\n",
    "npt.assert_array_almost_equal(classify(X_tst, param_tst,mode,drop_prob),[[1,0,1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Momentum\n",
    "\n",
    "A very popular technique that is used along with gradient descent is Momentum. Instead of using only the gradient of the current step to guide the search for minima, momentum also accumulates the gradient of the past steps to determine the direction of descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    '''\n",
    "    Inputs:\n",
    "        parameters - The Weight and Bias parameters of the network\n",
    "        \n",
    "    Outputs:\n",
    "        v - velocity parameter\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "            \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cbe913fb078bc8d945d29ea815db199",
     "grade": false,
     "grade_id": "cell-84af55196920d923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parameter updates\n",
    "\n",
    "The parameter gradients $(dW,db)$ calculated during back propagation are used to update the values of the network parameters.\n",
    "\n",
    "\\begin{equation}\n",
    "V_{t+1} = \\beta  V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\\n",
    "\\theta_{t+1} =\\theta_{t} -\\alpha(V_{t+1}), \\quad \\theta \\in \\{ W,b \\}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\alpha$ is the learning rate of the network and $\\beta$ is the momentum parameter . As discussed in the lecture, decay rate is used to adjust the learning rate smoothly across the gradient curve to avoid overshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe8297e38365b867978d37aefac224eb",
     "grade": false,
     "grade_id": "cell-749b76418a65d0a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, gradients, epoch, v, beta, learning_rate, decay_rate=0.01):\n",
    "    '''\n",
    "    Updates the network parameters with gradient descent\n",
    "\n",
    "    Inputs:\n",
    "        parameters - dictionary of network parameters \n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "        gradients - dictionary of gradient of network parameters \n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "        epoch - epoch number\n",
    "        v - Velocity parameter\n",
    "        beta - momentum parameter\n",
    "        learning_rate - step size for learning\n",
    "        decay_rate - rate of decay of step size - not necessary - in case you want to use\n",
    "    '''\n",
    "    \n",
    "    alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    for i in range(L):\n",
    "        w = parameters[\"W\"+str(i+1)]\n",
    "        b = parameters[\"W\"+str(i+1)]\n",
    "\n",
    "        dW = gradients[\"dW\"+str(i+1)]\n",
    "        db = gradients[\"db\"+str(i+1)]\n",
    "        \n",
    "        vw = v[\"dW\"+str(i+1)]\n",
    "        vb = v[\"db\"+str(i+1)]\n",
    "        \n",
    "        vw = beta*vw + (1-beta) * dW\n",
    "        w = w - alpha*vw\n",
    "        \n",
    "        vb = beta*vb + (1-beta) * db\n",
    "        b = b - alpha*vb\n",
    "        \n",
    "        v[\"dW\" + str(i + 1)] = vw\n",
    "        v[\"db\" + str(i + 1)] = vb\n",
    "        \n",
    "        parameters[\"W\"+str(i+1)] = w\n",
    "        parameters[\"b\"+str(i+1)] = b\n",
    "    \n",
    "    print(parameters)\n",
    "    return parameters, alpha, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c038bbf7001f64025ada3b72e551673c",
     "grade": true,
     "grade_id": "test_case9_backprop",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': 4.00990099009901, 'b1': 3.217821782178218, 'W2': 2.594059405940594, 'b2': -0.5742574257425748}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-c1648260a288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparam_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mal_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters_with_momentum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mparam_tst\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapprox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.399\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_tst = np.array([1,3,5,7])\n",
    "param_tst ={'W1':5,'b1':7,'W2':2,'b2':3}\n",
    "grad_tst ={'dW1':1,'db1':2,'dW2':-1,'db2':3}\n",
    "epoch_tst = 1\n",
    "v = {'dW1':1,'db1':1,'dW2':1,'db2':1}\n",
    "learning_rate_tst = 1\n",
    "decay_rate = 0.01\n",
    "beta = 0.2\n",
    "\n",
    "param_tst, al_tst, v_tst = update_parameters_with_momentum(param_tst, grad_tst, epoch_tst, v, beta, learning_rate_tst, decay_rate)\n",
    "\n",
    "assert param_tst == {'W1': 4.0, 'b1': 5.2, 'W2': 2.6, 'b2': pytest.approx(0.399,.01)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "484a27b0805a92d1e9b6511285d19e6b",
     "grade": false,
     "grade_id": "cell-35b5f7dedb40b1ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "842387a2365d1593405fed154fe50522",
     "grade": false,
     "grade_id": "cell-76abe4d415a1f55e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural Network (10 Points)\n",
    "\n",
    "Let us now assemble all the components of the neural network together and define a complete training loop for the Multi-layer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07990c2b08d66d97e0a7eb1875dcf4e0",
     "grade": false,
     "grade_id": "cell-f58ffc97952c7586",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_network(X, Y,valX, valY, net_dims, drop_prob, mode, num_iterations=500, learning_rate=0.2, decay_rate=0.01):\n",
    "    \n",
    "    '''\n",
    "    Creates the multilayer network and trains the network\n",
    "\n",
    "    Inputs:\n",
    "        X - numpy.ndarray (n,m) of training data\n",
    "        Y - numpy.ndarray (1,m) of training data labels\n",
    "        valX - numpy.ndarray(n,m) of validation data\n",
    "        valY - numpy.ndarray(1,m) of validation data labels\n",
    "        net_dims - tuple of layer dimensions\n",
    "        drop_prob - dropout parameter, we drop the number of neurons in a given layer with respect to prob.\n",
    "        mode - Takes in 2 values 'train' or 'test' mode. Model behaviour is dependent on the mode.\n",
    "        num_iterations - num of epochs to train\n",
    "        learning_rate - step size for gradient descent\n",
    "        decay_rate - the rate at which the learning rate is decayed.\n",
    "        \n",
    "    Returns:\n",
    "        costs - list of costs over training\n",
    "        val_costs - list of validation costs over training\n",
    "        parameters - dictionary of trained network parameters\n",
    "    '''\n",
    "\n",
    "    parameters = initialize(net_dims)\n",
    "    A0 = X\n",
    "    costs = []\n",
    "    val_costs = []\n",
    "    num_classes = 10\n",
    "    Y_one_hot = one_hot(Y,num_classes)\n",
    "    valY_one_hot = one_hot(valY,num_classes)\n",
    "    alpha = learning_rate\n",
    "    beta = 0.9\n",
    "    for ii in range(num_iterations):\n",
    "        \n",
    "        ## Forward Propagation (Training data)\n",
    "        # Step 1: Propagate the input A0 through the layers using multi_layer_forward()\n",
    "        # and calculate output of last layer Z and obtain cached activations as 'cache_1'\n",
    "        # Step 2: Compute the softmax activation AL, cross entropy cost and obtain the cached activation of last layer cache_2 using\n",
    "        # 'Z' from step 1 and 'Y_one_hot' (one hot representation of true class label)\n",
    "       \n",
    "        ## Back Propagation (with training data)\n",
    "        # Step 3: Compute the dervative of the softmax cross entropy loss (dZ) using one-hot encoded class labels 'Y_one_hot'\n",
    "        # and the cached activations cache_2 from forward pass.\n",
    "        # Step 4: Compute the parameter 'gradients' by passing dZ (from step 3) and cache1 from forward_pass to multi_layer_backward()\n",
    "        # Step 5: Initialize the velocity parameter by passing 'parameters' to 'initialize_velocity()'\n",
    "        # Step 6: Update the parameters of the network and obtain learning rate(alpha) by passing the calculated gradients to update_parameters_with_momentum() function.\n",
    "       \n",
    "        ## Forward Propagation (validation data only to estimate the loss)\n",
    "        # Step 7: Propagate the input valX through the layers using multi_layer_forward() with 'mode' ='test'\n",
    "        # and calculate output of last layer Z_ and obtain cached activations as 'cache_'\n",
    "        # Step 8: Compute the softmax activation AL, cross entropy cost for the validation set\n",
    "        # using 'Z_' from step 7 and 'Y_one_hot' (one hot representation of true class label)        \n",
    "    \n",
    "# YOUR CODE HERE\n",
    "        \n",
    "        #Forward prop(training data)\n",
    "        Z, cache_1 = multi_layer_forward(A0, parameters,drop_prob, mode)\n",
    "        AL, cache_2 ,loss =softmax_cross_entropy_loss(Z,Y_one_hot)\n",
    "        \n",
    "        #Backward prop(training data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Forward prop(training data)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raise NotImplementedError()\n",
    "    \n",
    "        if ii % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            val_costs.append(val_cost)\n",
    "        if ii % 10 == 0:\n",
    "            print(\"Cost at iteration %i is: %.05f, learning rate: %.05f\" %(ii, cost, alpha))\n",
    "    \n",
    "    return costs, val_costs, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7beb4d4a7c5fafe868bde17f67f47efd",
     "grade": true,
     "grade_id": "test_case10_complete",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43881a69e2334a62c8077d01335391ae",
     "grade": false,
     "grade_id": "cell-f44aae42add8fd84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Training\n",
    "We will now intialize a neural network with 2 hidden layers whose dimensions are 516 and 256 respectively.\n",
    "Since the input samples are of dimension 28 $\\times$ 28, the input layer will be of dimension 784. We will train the model and compute its accuracy on training, validation and test sets. Finally the training and validation cost is plotted against the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56ced067da0fb7d911eb5ffd88dfc894",
     "grade": false,
     "grade_id": "cell-870d6e0d01da6a52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Configuration 1 - Overfittting case, No dropout regularization\n",
    "\n",
    "net_dims = [784,516,256]\n",
    "net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "train_data, train_label, test_data, test_label, val_data, val_label = mnist(noTrSamples=train_samples,noValSamples= val_samples,noTsSamples=test_samples,digits= digits)\n",
    "\n",
    "# initialize learning rate and num_iterations\n",
    "learning_rate = .01\n",
    "num_iterations = 500\n",
    "\n",
    "drop_prob = 0\n",
    "mode = 'train'\n",
    "\n",
    "costs,val_costs, parameters = multi_layer_network(train_data, train_label,val_data, val_label, net_dims, drop_prob, mode, \\\n",
    "        num_iterations=num_iterations, learning_rate= learning_rate)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "\n",
    "\n",
    "mode ='test'\n",
    "train_Pred = classify(train_data, parameters,mode,drop_prob)\n",
    "val_Pred = classify(val_data, parameters,mode,drop_prob)\n",
    "test_Pred = classify(test_data, parameters,mode,drop_prob)\n",
    "print(train_Pred.shape)\n",
    "\n",
    "\n",
    "trAcc = ( 1 - np.count_nonzero(train_Pred - train_label ) / float(train_Pred.shape[1])) * 100 \n",
    "valAcc = ( 1 - np.count_nonzero(val_Pred - val_label ) / float(val_Pred.shape[1])) * 100 \n",
    "teAcc = ( 1 - np.count_nonzero(test_Pred - test_label ) / float(test_Pred.shape[1]) ) * 100\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for validation set is {0:0.3f} %\".format(valAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "X = range(0,num_iterations,10)\n",
    "plt.plot(X,costs)\n",
    "plt.plot(X,val_costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1415385d13fe0cb657f0f7074efd00e",
     "grade": false,
     "grade_id": "cell-d498cd08a0612eaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Configuration 2 - using dropout regularization\n",
    "\n",
    "net_dims = [784,516,256]\n",
    "net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "train_data, train_label, test_data, test_label, val_data, val_label = mnist(noTrSamples=train_samples,noValSamples= val_samples,noTsSamples=test_samples,digits= digits)\n",
    "\n",
    "# initialize learning rate and num_iterations\n",
    "learning_rate = .01\n",
    "num_iterations = 500\n",
    "\n",
    "drop_prob = 0.2\n",
    "mode = 'train'\n",
    "\n",
    "costs,val_costs, parameters = multi_layer_network(train_data, train_label,val_data, val_label, net_dims, drop_prob, mode, \\\n",
    "        num_iterations=num_iterations, learning_rate= learning_rate)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "mode ='test'\n",
    "train_Pred = classify(train_data, parameters,mode,drop_prob)\n",
    "val_Pred = classify(val_data, parameters,mode,drop_prob)\n",
    "test_Pred = classify(test_data, parameters,mode,drop_prob)\n",
    "print(train_Pred.shape)\n",
    "\n",
    "\n",
    "trAcc = ( 1 - np.count_nonzero(train_Pred - train_label ) / float(train_Pred.shape[1])) * 100 \n",
    "valAcc = ( 1 - np.count_nonzero(val_Pred - val_label ) / float(val_Pred.shape[1])) * 100 \n",
    "teAcc = ( 1 - np.count_nonzero(test_Pred - test_label ) / float(test_Pred.shape[1]) ) * 100\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for validation set is {0:0.3f} %\".format(valAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "X = range(0,num_iterations,10)\n",
    "plt.plot(X,costs)\n",
    "plt.plot(X,val_costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "129b655e8f83d0895219d63803cd86b1",
     "grade": false,
     "grade_id": "cell-09dfb954841c8d7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Extra Credit (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b748697f477a3d8c4a72789723a878ac",
     "grade": false,
     "grade_id": "cell-15e856b35358f002",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Softmax Cross Entropy and Entropy Loss\n",
    "\n",
    "Let $Z \\in \\mathbb{R}^{d \\times m}$ be the input to the loss function, \n",
    "where $Z = [z^{(1)}, z^{(2)}, \\ldots, z^{(m)}]$ \n",
    "with $z^{(i)}\\in \\mathbb{R}^{d}$ as the $i^{th}$ sample. \n",
    "$d$ is the dimension of each of the $m$ samples. \n",
    "Let $Y \\in \\mathbb{R}^{1 \\times m}$ be the corresponding labels for samples in $Z$ \n",
    "with $Y = [y^{(1)}, y^{(2)}, \\ldots, y^{(m)}]$. \n",
    "Unlabeled samples have $y^{(i)} = -1$ and labeled samples have $y^{(i)} \\in \\{0,1,\\ldots,C\\}$. \n",
    "Let the number of labeled samples be $m_l$ and number of unlabeled samples be $m_u$\n",
    "with, $m = m_l + m_u$. \n",
    "Let $A \\in \\mathbb{R}^{d \\times m}$ where $A = [a^{(1)}, a^{(2)}, \\ldots, a^{(m)}]$ \n",
    "where $a^{(i)}\\in \\mathbb{R}^{d}$ is the softmax activation for $z^{(i)}$. \n",
    "Since it is the softmax activation, $d = C+1$.  \n",
    "Let $A_l \\subset A$ and $A_u \\subset A$ be the subsets of softmax activations for the labeled \n",
    "and unlabeled samples respectively. \n",
    "The loss function is given by \n",
    "\\begin{equation}\n",
    "L(A,Y) = -\\frac{1}{m_l}\\sum_{a^{(i)}\\in A_l}\\sum_{k=0}^C\\text{I}\\{y^{(i)}=k\\}\\text{log}a_k^{(i)}\n",
    "     -\\frac{1}{m_u}\\sum_{a^{(j)}\\in A_u}\\sum_{k=0}^C a_k^{(j)}\\text{log}a_k^{(j)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e68b348d6f97d5dec6369a9895f1d298",
     "grade": false,
     "grade_id": "cell-a0f1c6d98d7c176e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_and_entropy_loss(Y,Z):\n",
    "    '''\n",
    "    The loss function is sum of cross-entropy loss and entropy loss. \n",
    "    Apply the softmax activation to Z, say A = softmax(Z)\n",
    "    Then, estimate the cross_entropy loss for the labeled samples in A \n",
    "    and entropy loss for unlabeled samples in A \n",
    "\n",
    "    Input:  Y is of dimensions (1 x m) where Yi is in {0,1,2,...C,-1}\n",
    "            Yi is {0,1,2,...C} for labeled samples and Yi is {-1} for unlabeled samples\n",
    "            Z is of dimensions (d x m) the input to the loss layer, \n",
    "            where d is dimensions and m is number of samples \n",
    "            Since it is the softmax activation, d = C+1\n",
    "    Output: loss - the cross entropy loss for the labeled samples + \n",
    "            the entropy loss for the unlabeled samples\n",
    "            A - the softmax activation for Z \n",
    "            cache - a dictionary with cache[\"Z\"] = Z\n",
    "    '''\n",
    "    # Hint: Apply softmax activation for all data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #Total loss is cross entropy loss + entropy loss \n",
    "    loss = ce_loss + e_loss\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "223ef86a9e9a65ce51cd1b56c94a4c25",
     "grade": false,
     "grade_id": "cell-941d12eb44e5aac2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_and_entropy_loss_der(Y, cache):\n",
    "    '''\n",
    "    Estimate the derivative dZ = dL/dZ where Z is the input to the \n",
    "    softmax_cross_entropy_and_entropy_loss function\n",
    "\n",
    "    Input:  Y is of dimensions (1 x m) where Yi is in {0,1,2,...C,-1}\n",
    "            Yi is {0,1,2,...C} for labeled samples and Yi is {-1} for unlabeled samples\n",
    "            cache - a dictionary with cache[\"Z\"] = Z\n",
    "            where, Z is of dimensions (d x m) the input to the loss layer, \n",
    "            where d is dimensions and m is number of samples \n",
    "            Since it is the softmax activation, d = C+1\n",
    "    Output: dZ - derivative of the softmax_cross_entropy_and_entropy_loss w.r.t. Z\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe3f597e6e9dc353acd0f3313b78ae8b",
     "grade": false,
     "grade_id": "cell-0076063d8691df6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradientCheck(Y,Z):\n",
    "    '''\n",
    "    Verify if the numerical estimate of the gradient is the same as the empirical estimate \n",
    "    Let eps = 1e-4, then estimate numerical derivative numdZ as (loss(Z+eps) - loss(Z-eps))/(2*eps) \n",
    "    The difference between the numerical gradient numdZ and \n",
    "    the empirical gradient dZ should be less the 1e-7\n",
    "\n",
    "    Input:  Y is of dimensions (1 x m) where Yi is in {0,1,2,...C,-1}\n",
    "            Yi is {0,1,2,...C} for labeled samples and Yi is {-1} for unlabeled samples\n",
    "            Z is of dimensions (d x m) the input to the loss layer, \n",
    "            where d is dimensions and m is number of samples \n",
    "            Since it is the softmax activation, d = C+1\n",
    "    Output: diff - the normalized difference between the numerical gradient and the empirical gradient\n",
    "    \n",
    "    Reference: https://www.youtube.com/watch?v=P6EtCVrvYPU  \n",
    "    '''\n",
    "    _, cache, _ = softmax_cross_entropy_and_entropy_loss(Y,Z)\n",
    "    dZ = softmax_cross_entropy_and_entropy_loss_der(Y, cache)\n",
    "    EPSILON = 1e-4\n",
    "    numdZ = np.zeros_like(Z)\n",
    "    for idx, _ in np.ndenumerate(Z):\n",
    "        delta = np.zeros_like(Z)\n",
    "        delta[idx] = 1\n",
    "        Z_plus = Z + delta*EPSILON\n",
    "        Z_minus = Z - delta*EPSILON\n",
    "        _, _, l_plus = softmax_cross_entropy_and_entropy_loss(Y,Z_plus)\n",
    "        _, _, l_minus = softmax_cross_entropy_and_entropy_loss(Y,Z_minus)\n",
    "        numdZ[idx] = (l_plus - l_minus)/(2*EPSILON)\n",
    "    \n",
    "    diff = np.linalg.norm(numdZ-dZ)/np.linalg.norm(numdZ+dZ)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07a89be816cb4c28f08d37f2300cdaf7",
     "grade": true,
     "grade_id": "extra_credit",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Z_tst = np.array([[1., 2., 3., 4., 5., 6., 7.],[2., 3., 4., 5., 6., 7., 8.],[3., 4., 5., 6., 7., 8., 9.]])\n",
    "Y_tst = np.array([[-1, 1, 2, 0, -1, -1, -1]])\n",
    "ce_loss_res = 1.4076059087886696\n",
    "e_loss_res = 0.8323955518399399\n",
    "A_tst, cache_tst, loss_tst = softmax_cross_entropy_and_entropy_loss(Y_tst,Z_tst)\n",
    "npt.assert_almost_equal(loss_tst,(ce_loss_res + e_loss_res))\n",
    "\n",
    "dZ_res = [[ 0.03545427,  0.03001019,  0.03001019, -0.30332314,  0.03545427,  0.03545427, 0.03545427],\n",
    "          [ 0.03519259, -0.25175718,  0.08157616,  0.08157616,  0.03519259,  0.03519259, 0.03519259],\n",
    "          [-0.07064686,  0.22174699, -0.11158635,  0.22174699, -0.07064686, -0.07064686, -0.07064686]]\n",
    "\n",
    "dZ_tst = softmax_cross_entropy_and_entropy_loss_der(Y_tst, cache_tst)\n",
    "npt.assert_almost_equal(dZ_tst,dZ_res,3)\n",
    "\n",
    "cache_tst = {}\n",
    "cache_tst[\"Z\"] = Z_tst\n",
    "diff = gradientCheck(Y_tst,Z_tst)\n",
    "assert diff <= 1e-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsl_env",
   "language": "python",
   "name": "fsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
